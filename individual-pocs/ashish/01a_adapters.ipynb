{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhGnfCq5uml4bHrGenbroY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Auto-detects each dataset’s raw schema.\n","\n","Maps to standardized columns for NLP and transactions:\n","\n","* Text corpora: text, label, intent, slots, conversation_id, turn_id, user_id, timestamp, source\n","\n","* Transactions: invoice, item, description, qty, timestamp, price, user_id, country, source\n","\n","\n","Prints:\n","\n","* Found columns per dataset\n","\n","* Mapping used per dataset\n","\n","* Head() of standardized frame\n","\n","Visualizes:\n","\n","* Label distribution (if present)\n","\n","* Basic text length histograms (if text present)\n","\n","Saves parquet: audit_outputs/standardized/{dataset}.parquet"],"metadata":{"id":"9Rl_XzH6fqh4"}},{"cell_type":"markdown","source":["Quick schema cheat-sheet (expected fields by source)\n","Relational strategies in customer service\n","\n","Text-like: text/utterance/message\n","\n","Label-like: label/strategy/category\n","\n","Optional: conversation_id, turn_id, user_id, timestamp\n","\n","3K conversations dataset for chatbot\n","\n","Text-like: text/utterance/message\n","\n","Optional: intent/label/slots, conversation_id/dialogue_id, turn_id, user/speaker, timestamp\n","\n","Customer support on Twitter\n","\n","Typical: text, created_at/timestamp, author_id/user_id, conversation/thread IDs; sometimes label/intent absent\n","\n","Stanford Sentiment Treebank\n","\n","sentence/text/phrase, label (binary or 5-class index/prob bins)\n","\n","SNIPS NLU\n","\n","utterance, intent, slots\n","\n","Topic modeling with BERT input\n","\n","A single text/content column; unlabeled docs\n","\n","Reuters-21578\n","\n","text/body, topics (multi-label), optional title/date/split\n","\n","E-commerce data (Online Retail)\n","\n","InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country\n","\n","Opendatabay consumer datasets\n","\n","Either transaction-like (invoice, item, qty, price, timestamp, customer, country) or text/review-like (text, rating/label)"],"metadata":{"id":"gN4V73YSiLYY"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"953U9aLgfj7Q","executionInfo":{"status":"ok","timestamp":1754889076481,"user_tz":-330,"elapsed":2628,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"outputs":[],"source":["# Setup\n","import os, re, json, glob, warnings\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","warnings.filterwarnings(\"ignore\")\n","sns.set(style=\"whitegrid\")\n","\n","os.makedirs(\"audit_outputs/standardized\", exist_ok=True)\n"]},{"cell_type":"code","source":["# Shared helpers\n","\n","def savefig(path):\n","    plt.tight_layout()\n","    plt.savefig(path, dpi=160)\n","    plt.close()\n","\n","def print_header(title):\n","    print(\"\\n\" + \"=\"*90)\n","    print(title)\n","    print(\"=\"*90)\n","\n","def show_schema(df, name):\n","    print_header(f\"{name} | raw schema\")\n","    print(f\"Shape: {df.shape}\")\n","    print(\"Columns:\", list(df.columns))\n","    display(df.head(3))\n","\n","def plot_label_dist(df, label_col, name):\n","    counts = df[label_col].value_counts()\n","    plt.figure(figsize=(10,4))\n","    sns.barplot(x=counts.index, y=counts.values, palette=\"Blues_d\")\n","    plt.title(f\"{name}: {label_col} distribution\")\n","    plt.xticks(rotation=45, ha=\"right\")\n","    savefig(f\"audit_outputs/standardized/{name}_label_distribution.png\")\n","    print(f\"Saved label distribution plot -> audit_outputs/standardized/{name}_label_distribution.png\")\n","\n","def plot_text_lengths(df, text_col, name):\n","    lens = df[text_col].fillna(\"\").astype(str).str.split().map(len)\n","    plt.figure(figsize=(8,4))\n","    sns.histplot(lens, bins=60, kde=False)\n","    plt.title(f\"{name}: token length histogram\")\n","    savefig(f\"audit_outputs/standardized/{name}_text_lengths.png\")\n","    print(f\"Saved text length plot -> audit_outputs/standardized/{name}_text_lengths.png\")\n"],"metadata":{"id":"4BWEM5IogV2P","executionInfo":{"status":"ok","timestamp":1754889082834,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Unified schema and mapping utilities\n","\n","# Target columns for NLP corpora\n","NLP_TARGET = [\"text\",\"label\",\"intent\",\"slots\",\"conversation_id\",\"turn_id\",\"user_id\",\"timestamp\",\"source\"]\n","\n","# Target columns for transactions\n","TX_TARGET  = [\"invoice\",\"item\",\"description\",\"qty\",\"timestamp\",\"price\",\"user_id\",\"country\",\"source\"]\n","\n","def map_first_present(d, candidates, default=None):\n","    for c in candidates:\n","        if c in d.columns:\n","            return c\n","    return default\n","\n","def ensure_cols(df, cols):\n","    out = df.copy()\n","    for c in cols:\n","        if c not in out.columns:\n","            out[c] = np.nan\n","    return out[cols]\n"],"metadata":{"id":"ed2xaTxCgeTs","executionInfo":{"status":"ok","timestamp":1754889104220,"user_tz":-330,"elapsed":40,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Adapters for each dataset\n","# Note: Paths assume you’ve downloaded/unzipped raw files under data/raw/{shortname}/. Adjust file patterns as needed."],"metadata":{"id":"DkDL1KXNgmOM","executionInfo":{"status":"ok","timestamp":1754889111998,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Relational strategies in customer service\n","\n","def load_relational_strategies(base=\"data/raw/relational_strategies\"):\n","    # Try CSV/JSONL variants; flexible mapping\n","    files = glob.glob(os.path.join(base, \"*.csv\")) + glob.glob(os.path.join(base, \"*.jsonl\"))\n","    if not files:\n","        print(f\"No files found under {base}\")\n","        return pd.DataFrame(columns=NLP_TARGET)\n","    frames = []\n","    for f in files:\n","        if f.endswith(\".csv\"):\n","            df = pd.read_csv(f, encoding_errors=\"ignore\")\n","        else:\n","            df = pd.read_json(f, lines=True)\n","        df[\"source\"] = \"relational_strategies\"\n","        frames.append(df)\n","    raw = pd.concat(frames, ignore_index=True)\n","    show_schema(raw, \"relational_strategies\")\n","\n","    # Candidate columns\n","    text_cand   = [\"text\",\"utterance\",\"message\",\"content\",\"body\"]\n","    label_cand  = [\"label\",\"strategy\",\"tag\",\"category\"]\n","    intent_cand = [\"intent\",\"intent_label\"]\n","    slots_cand  = [\"slots\",\"entities\"]\n","    conv_cand   = [\"conversation_id\",\"dialogue_id\",\"thread_id\",\"conv_id\"]\n","    turn_cand   = [\"turn_id\",\"turn\",\"message_id\",\"seq\"]\n","    user_cand   = [\"user_id\",\"author_id\",\"customer_id\",\"uid\"]\n","    time_cand   = [\"timestamp\",\"created_at\",\"time\",\"date\"]\n","\n","    cols = {\n","        \"text\": map_first_present(raw, text_cand),\n","        \"label\": map_first_present(raw, label_cand),\n","        \"intent\": map_first_present(raw, intent_cand),\n","        \"slots\": map_first_present(raw, slots_cand),\n","        \"conversation_id\": map_first_present(raw, conv_cand),\n","        \"turn_id\": map_first_present(raw, turn_cand),\n","        \"user_id\": map_first_present(raw, user_cand),\n","        \"timestamp\": map_first_present(raw, time_cand),\n","    }\n","    print_header(\"relational_strategies | column mapping\")\n","    print(cols)\n","\n","    std = pd.DataFrame()\n","    for k,v in cols.items():\n","        std[k] = raw[v] if v in raw.columns else np.nan\n","    std[\"source\"] = \"relational_strategies\"\n","    std = ensure_cols(std, NLP_TARGET)\n","\n","    if std[\"label\"].notna().any():\n","        plot_label_dist(std, \"label\", \"relational_strategies\")\n","    if std[\"text\"].notna().any():\n","        plot_text_lengths(std, \"text\", \"relational_strategies\")\n","\n","    out_path = \"audit_outputs/standardized/relational_strategies.parquet\"\n","    std.to_parquet(out_path, index=False)\n","    print(\"Saved standardized:\", out_path)\n","    return std\n"],"metadata":{"id":"UjPwuiIRgzWz","executionInfo":{"status":"ok","timestamp":1754889113483,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 3K conversations dataset for chatbot\n","\n","def load_conversations_3k(base=\"data/raw/conversations_3k\"):\n","    files = glob.glob(os.path.join(base, \"*.csv\")) + glob.glob(os.path.join(base, \"*.jsonl\")) + glob.glob(os.path.join(base, \"*.json\"))\n","    if not files:\n","        print(f\"No files found under {base}\")\n","        return pd.DataFrame(columns=NLP_TARGET)\n","    frames=[]\n","    for f in files:\n","        if f.endswith(\".csv\"):\n","            frames.append(pd.read_csv(f, encoding_errors=\"ignore\"))\n","        elif f.endswith(\".jsonl\"):\n","            frames.append(pd.read_json(f, lines=True))\n","        else:\n","            frames.append(pd.read_json(f))\n","    raw = pd.concat(frames, ignore_index=True)\n","    raw[\"source\"] = \"conversations_3k\"\n","    show_schema(raw, \"conversations_3k\")\n","\n","    text_cand   = [\"text\",\"utterance\",\"message\",\"content\"]\n","    label_cand  = [\"label\",\"category\",\"tag\"]\n","    intent_cand = [\"intent\",\"intent_label\"]\n","    slots_cand  = [\"slots\",\"entities\"]\n","    conv_cand   = [\"conversation_id\",\"dialogue_id\",\"thread_id\",\"conv_id\",\"dialogueID\"]\n","    turn_cand   = [\"turn_id\",\"turn\",\"message_id\",\"seq\",\"index\"]\n","    user_cand   = [\"user_id\",\"author_id\",\"customer_id\",\"uid\",\"speaker_id\",\"speaker\"]\n","    time_cand   = [\"timestamp\",\"created_at\",\"time\",\"date\"]\n","\n","    cols = {\n","        \"text\": map_first_present(raw, text_cand),\n","        \"label\": map_first_present(raw, label_cand),\n","        \"intent\": map_first_present(raw, intent_cand),\n","        \"slots\": map_first_present(raw, slots_cand),\n","        \"conversation_id\": map_first_present(raw, conv_cand),\n","        \"turn_id\": map_first_present(raw, turn_cand),\n","        \"user_id\": map_first_present(raw, user_cand),\n","        \"timestamp\": map_first_present(raw, time_cand),\n","    }\n","    print_header(\"conversations_3k | column mapping\")\n","    print(cols)\n","\n","    std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","    std[\"source\"] = \"conversations_3k\"\n","    std = ensure_cols(std, NLP_TARGET)\n","\n","    if std[\"label\"].notna().any():\n","        plot_label_dist(std, \"label\", \"conversations_3k\")\n","    if std[\"text\"].notna().any():\n","        plot_text_lengths(std, \"text\", \"conversations_3k\")\n","\n","    out_path = \"audit_outputs/standardized/conversations_3k.parquet\"\n","    std.to_parquet(out_path, index=False)\n","    print(\"Saved standardized:\", out_path)\n","    return std\n"],"metadata":{"id":"982Jsjlsg5nU","executionInfo":{"status":"ok","timestamp":1754889119413,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Customer support on Twitter\n","\n","def load_twitter_support(base=\"data/raw/twitter_support\"):\n","    files = glob.glob(os.path.join(base, \"*.csv\"))\n","    if not files:\n","        print(f\"No files found under {base}\")\n","        return pd.DataFrame(columns=NLP_TARGET)\n","    raw = pd.concat([pd.read_csv(f, encoding_errors=\"ignore\") for f in files], ignore_index=True)\n","    raw[\"source\"] = \"twitter_support\"\n","    show_schema(raw, \"twitter_support\")\n","\n","    # Known-ish fields in popular releases\n","    text_cand   = [\"text\",\"tweet_text\",\"body\"]\n","    time_cand   = [\"created_at\",\"timestamp\",\"time\"]\n","    user_cand   = [\"author_id\",\"user_id\",\"userid\"]\n","    conv_cand   = [\"conversation_id\",\"in_response_to_tweet_id\",\"thread_id\"]\n","    turn_cand   = [\"turn_id\",\"seq\"]\n","    label_cand  = [\"label\",\"category\",\"tag\"]\n","    intent_cand = [\"intent\",\"intent_label\"]\n","    slots_cand  = [\"slots\",\"entities\"]\n","\n","    cols = {\n","        \"text\": map_first_present(raw, text_cand),\n","        \"label\": map_first_present(raw, label_cand),\n","        \"intent\": map_first_present(raw, intent_cand),\n","        \"slots\": map_first_present(raw, slots_cand),\n","        \"conversation_id\": map_first_present(raw, conv_cand),\n","        \"turn_id\": map_first_present(raw, turn_cand),\n","        \"user_id\": map_first_present(raw, user_cand),\n","        \"timestamp\": map_first_present(raw, time_cand),\n","    }\n","    print_header(\"twitter_support | column mapping\")\n","    print(cols)\n","\n","    std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","    std[\"source\"] = \"twitter_support\"\n","    std = ensure_cols(std, NLP_TARGET)\n","\n","    if std[\"label\"].notna().any():\n","        plot_label_dist(std, \"label\", \"twitter_support\")\n","    if std[\"text\"].notna().any():\n","        plot_text_lengths(std, \"text\", \"twitter_support\")\n","\n","    out_path = \"audit_outputs/standardized/twitter_support.parquet\"\n","    std.to_parquet(out_path, index=False)\n","    print(\"Saved standardized:\", out_path)\n","    return std\n"],"metadata":{"id":"Kmq0e1crg_t9","executionInfo":{"status":"ok","timestamp":1754889123021,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Stanford Sentiment Treebank (SST)\n","\n","def load_sst(base=\"data/raw/sst\"):\n","    # Support tsv/csv with sentence or text and label\n","    files = glob.glob(os.path.join(base, \"*.tsv\")) + glob.glob(os.path.join(base, \"*.csv\"))\n","    if not files:\n","        print(f\"No files found under {base}\")\n","        return pd.DataFrame(columns=NLP_TARGET)\n","    frames=[]\n","    for f in files:\n","        if f.endswith(\".tsv\"):\n","            frames.append(pd.read_csv(f, sep=\"\\t\", encoding_errors=\"ignore\"))\n","        else:\n","            frames.append(pd.read_csv(f, encoding_errors=\"ignore\"))\n","    raw = pd.concat(frames, ignore_index=True)\n","    raw[\"source\"] = \"sst\"\n","    show_schema(raw, \"sst\")\n","\n","    text_cand   = [\"sentence\",\"text\",\"phrase\"]\n","    label_cand  = [\"label\",\"sentiment\",\"sentiment_values\",\"class\"]\n","    time_cand   = []\n","    user_cand   = []\n","    cols = {\n","        \"text\": map_first_present(raw, text_cand),\n","        \"label\": map_first_present(raw, label_cand),\n","        \"intent\": None, \"slots\": None,\n","        \"conversation_id\": None, \"turn_id\": None,\n","        \"user_id\": None, \"timestamp\": None\n","    }\n","    print_header(\"sst | column mapping\")\n","    print(cols)\n","\n","    std = pd.DataFrame()\n","    for k,v in cols.items():\n","        std[k] = raw[v] if v and v in raw.columns else np.nan\n","    std[\"source\"] = \"sst\"\n","    std = ensure_cols(std, NLP_TARGET)\n","\n","    if std[\"label\"].notna().any():\n","        plot_label_dist(std, \"label\", \"sst\")\n","    if std[\"text\"].notna().any():\n","        plot_text_lengths(std, \"text\", \"sst\")\n","\n","    out_path = \"audit_outputs/standardized/sst.parquet\"\n","    std.to_parquet(out_path, index=False)\n","    print(\"Saved standardized:\", out_path)\n","    return std\n"],"metadata":{"id":"rS1CFYadhGOo","executionInfo":{"status":"ok","timestamp":1754889128027,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# SNIPS NLU benchmark\n","\n","def load_snips(base=\"data/raw/snips\"):\n","    files = glob.glob(os.path.join(base, \"*.jsonl\")) + glob.glob(os.path.join(base, \"*.json\"))\n","    if not files:\n","        print(f\"No files found under {base}\")\n","        return pd.DataFrame(columns=NLP_TARGET)\n","    frames=[]\n","    for f in files:\n","        if f.endswith(\".jsonl\"):\n","            frames.append(pd.read_json(f, lines=True))\n","        else:\n","            frames.append(pd.read_json(f))\n","    raw = pd.concat(frames, ignore_index=True)\n","    raw[\"source\"] = \"snips\"\n","    show_schema(raw, \"snips\")\n","\n","    text_cand   = [\"utterance\",\"text\",\"sentence\"]\n","    intent_cand = [\"intent\",\"intentName\",\"intent_name\"]\n","    slots_cand  = [\"slots\",\"entities\"]\n","    label_cand  = [\"label\",\"category\"]  # often no direct \"label\", intent is target\n","    cols = {\n","        \"text\": map_first_present(raw, text_cand),\n","        \"label\": map_first_present(raw, label_cand),\n","        \"intent\": map_first_present(raw, intent_cand),\n","        \"slots\": map_first_present(raw, slots_cand),\n","        \"conversation_id\": None,\n","        \"turn_id\": None,\n","        \"user_id\": None,\n","        \"timestamp\": None\n","    }\n","    print_header(\"snips | column mapping\")\n","    print(cols)\n","\n","    std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","    std[\"source\"] = \"snips\"\n","    std = ensure_cols(std, NLP_TARGET)\n","\n","    if std[\"intent\"].notna().any():\n","        plot_label_dist(std.rename(columns={\"intent\":\"_intent\"}), \"_intent\", \"snips_intent\")\n","    if std[\"text\"].notna().any():\n","        plot_text_lengths(std, \"text\", \"snips\")\n","\n","    out_path = \"audit_outputs/standardized/snips.parquet\"\n","    std.to_parquet(out_path, index=False)\n","    print(\"Saved standardized:\", out_path)\n","    return std\n"],"metadata":{"id":"Y_TqI_gphM5P","executionInfo":{"status":"ok","timestamp":1754889141232,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Topic modeling with BERT (inputs)\n","\n","def load_topic_inputs(base=\"data/raw/topic_inputs\"):\n","    # Treat as unlabeled docs; expect a CSV with a text-like column or TXT files\n","    csvs = glob.glob(os.path.join(base, \"*.csv\"))\n","    txts = glob.glob(os.path.join(base, \"*.txt\"))\n","    frames=[]\n","    if csvs:\n","        for f in csvs:\n","            df = pd.read_csv(f, encoding_errors=\"ignore\")\n","            frames.append(df)\n","    elif txts:\n","        rows=[]\n","        for f in txts:\n","            with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n","                for line in fh:\n","                    rows.append({\"text\": line.strip()})\n","        frames.append(pd.DataFrame(rows))\n","    else:\n","        print(f\"No files found under {base}\")\n","        return pd.DataFrame(columns=NLP_TARGET)\n","    raw = pd.concat(frames, ignore_index=True)\n","    raw[\"source\"] = \"topic_inputs\"\n","    show_schema(raw, \"topic_inputs\")\n","\n","    text_cand = [\"text\",\"content\",\"body\",\"headline\",\"headline_text\"]\n","    cols = {\n","        \"text\": map_first_present(raw, text_cand),\n","        \"label\": None, \"intent\": None, \"slots\": None,\n","        \"conversation_id\": None, \"turn_id\": None,\n","        \"user_id\": None, \"timestamp\": None\n","    }\n","    print_header(\"topic_inputs | column mapping\")\n","    print(cols)\n","\n","    std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","    std[\"source\"] = \"topic_inputs\"\n","    std = ensure_cols(std, NLP_TARGET)\n","\n","    if std[\"text\"].notna().any():\n","        plot_text_lengths(std, \"text\", \"topic_inputs\")\n","\n","    out_path = \"audit_outputs/standardized/topic_inputs.parquet\"\n","    std.to_parquet(out_path, index=False)\n","    print(\"Saved standardized:\", out_path)\n","    return std\n"],"metadata":{"id":"hFqoyhrRhShA","executionInfo":{"status":"ok","timestamp":1754889155464,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ashish Pathak","userId":"15139269617937724867"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Reuters-21578 (prefer preprocessed CSV)\n","\n","def load_reuters(base=\"data/raw/reuters21578\"):\n","    # Prefer CSV pre-extractions. If only SGM files exist, add a separate parser later.\n","    files = glob.glob(os.path.join(base, \"*.csv\"))\n","    if not files:\n","        print(f\"No CSV files found under {base}. If you have SGM, preprocess to CSV first.\")\n","        return pd.DataFrame(columns=NLP_TARGET)\n","    raw = pd.concat([pd.read_csv(f, encoding_errors=\"ignore\") for f in files], ignore_index=True)\n","    raw[\"source\"] = \"reuters21578\"\n","    show_schema(raw, \"reuters21578\")\n","\n","    text_cand   = [\"text\",\"body\",\"BODY\"]\n","    topics_cand = [\"topics\",\"TOPICS\"]\n","    title_cand  = [\"title\",\"TITLE\"]\n","    date_cand   = [\"date\",\"DATE\"]\n","    cols = {\n","        \"text\": map_first_present(raw, text_cand),\n","        \"label\": map_first_present(raw, topics_cand),  # multi-label possible; keep as is\n","        \"intent\": None, \"slots\": None,\n","        \"conversation_id\": None, \"turn_id\": None,\n","        \"user_id\": None, \"timestamp\": map_first_present(raw, date_cand)\n","    }\n","    print_header(\"reuters21578 | column mapping\")\n","    print(cols)\n","\n","    std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","    std[\"source\"] = \"reuters21578\"\n","    std = ensure_cols(std, NLP_TARGET)\n","\n","    if std[\"label\"].notna().any():\n","        # Flatten if stringified lists\n","        labels = std[\"label\"].dropna().astype(str).str.split(\",\")\n","        flat = pd.Series([x.strip() for sub in labels for x in sub if isinstance(sub, list) or isinstance(sub, list) == False])\n","        counts = flat.value_counts().head(20)\n","        plt.figure(figsize=(10,4))\n","        sns.barplot(x=counts.index, y=counts.values, palette=\"Greens_d\")\n","        plt.xticks(rotation=45, ha=\"right\")\n","        plt.title(\"reuters21578: top topics\")\n","        savefig(\"audit_outputs/standardized/reuters21578_topics.png\")\n","        print(\"Saved topics plot -> audit_outputs/standardized/reuters21578_topics.png\")\n","\n","    if std[\"text\"].notna().any():\n","        plot_text_lengths(std, \"text\", \"reuters21578\")\n","\n","    out_path = \"audit_outputs/standardized/reuters21578.parquet\"\n","    std.to_parquet(out_path, index=False)\n","    print(\"Saved standardized:\", out_path)\n","    return std\n"],"metadata":{"id":"NJPpChNhhbCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# E-commerce data (Online Retail)\n","\n","def load_ecommerce(base=\"data/raw/ecommerce\"):\n","    files = glob.glob(os.path.join(base, \"*.csv\")) + glob.glob(os.path.join(base, \"*.xlsx\"))\n","    if not files:\n","        print(f\"No files found under {base}\")\n","        return pd.DataFrame(columns=TX_TARGET)\n","    frames=[]\n","    for f in files:\n","        if f.endswith(\".csv\"):\n","            frames.append(pd.read_csv(f, encoding_errors=\"ignore\"))\n","        else:\n","            frames.append(pd.read_excel(f))\n","    raw = pd.concat(frames, ignore_index=True)\n","    raw[\"source\"] = \"ecommerce\"\n","    show_schema(raw, \"ecommerce\")\n","\n","    invoice_cand = [\"InvoiceNo\",\"invoice\"]\n","    item_cand    = [\"StockCode\",\"item\",\"SKU\",\"product_id\"]\n","    desc_cand    = [\"Description\",\"item_desc\",\"Product_Description\",\"description\"]\n","    qty_cand     = [\"Quantity\",\"qty\"]\n","    time_cand    = [\"InvoiceDate\",\"timestamp\",\"Date\",\"Transaction_Date\"]\n","    price_cand   = [\"UnitPrice\",\"price\",\"Price\",\"Purchase_Amount\"]\n","    user_cand    = [\"CustomerID\",\"user_id\",\"Customer_ID\"]\n","    country_cand = [\"Country\",\"country\",\"Location\"]\n","\n","    cols = {\n","        \"invoice\": map_first_present(raw, invoice_cand),\n","        \"item\": map_first_present(raw, item_cand),\n","        \"description\": map_first_present(raw, desc_cand),\n","        \"qty\": map_first_present(raw, qty_cand),\n","        \"timestamp\": map_first_present(raw, time_cand),\n","        \"price\": map_first_present(raw, price_cand),\n","        \"user_id\": map_first_present(raw, user_cand),\n","        \"country\": map_first_present(raw, country_cand),\n","    }\n","    print_header(\"ecommerce | column mapping\")\n","    print(cols)\n","\n","    std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","    std[\"source\"] = \"ecommerce\"\n","    std = ensure_cols(std, TX_TARGET)\n","\n","    print_header(\"ecommerce | quick numeric sanity\")\n","    for c in [\"qty\",\"price\"]:\n","        if c in std.columns:\n","            std[c] = pd.to_numeric(std[c], errors=\"coerce\")\n","            print(c, \"summary:\", std[c].describe())\n","\n","    out_path = \"audit_outputs/standardized/ecommerce.parquet\"\n","    std.to_parquet(out_path, index=False)\n","    print(\"Saved standardized:\", out_path)\n","    return std\n"],"metadata":{"id":"-kUyqjphhjh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Opendatabay consumer dataset A (schema discovery)\n","\n","def load_opendatabay_a(base=\"data/raw/opendatabay_a\"):\n","    files = glob.glob(os.path.join(base, \"*.csv\")) + glob.glob(os.path.join(base, \"*.json\"))\n","    if not files:\n","        print(f\"No files found under {base}\")\n","        return pd.DataFrame()\n","    frames=[]\n","    for f in files:\n","        if f.endswith(\".csv\"):\n","            frames.append(pd.read_csv(f, encoding_errors=\"ignore\"))\n","        else:\n","            frames.append(pd.read_json(f))\n","    raw = pd.concat(frames, ignore_index=True)\n","    raw[\"source\"] = \"opendatabay_a\"\n","    show_schema(raw, \"opendatabay_a\")\n","\n","    # Heuristic: if looks like transactions, map TX schema; else, try NLP schema\n","    lower_cols = [c.lower() for c in raw.columns]\n","    looks_tx = any(k in lower_cols for k in [\"invoice\",\"invoiceno\",\"stockcode\",\"quantity\",\"unitprice\",\"customerid\",\"country\",\"transaction_date\",\"purchase_amount\"])\n","    if looks_tx:\n","        invoice_cand = [\"InvoiceNo\",\"invoice\"]\n","        item_cand    = [\"StockCode\",\"item\",\"SKU\",\"product_id\",\"Item Purchased\"]\n","        desc_cand    = [\"Description\",\"item_desc\",\"Product_Description\",\"description\",\"Item Purchased\"]\n","        qty_cand     = [\"Quantity\",\"qty\"]\n","        time_cand    = [\"InvoiceDate\",\"timestamp\",\"Date\",\"Transaction_Date\",\"Transaction Date\"]\n","        price_cand   = [\"UnitPrice\",\"price\",\"Price\",\"Purchase_Amount\",\"Purchase Amount (USD)\"]\n","        user_cand    = [\"CustomerID\",\"user_id\",\"Customer_ID\",\"Customer ID\"]\n","        country_cand = [\"Country\",\"country\",\"Location\"]\n","\n","        cols = {\n","            \"invoice\": map_first_present(raw, invoice_cand),\n","            \"item\": map_first_present(raw, item_cand),\n","            \"description\": map_first_present(raw, desc_cand),\n","            \"qty\": map_first_present(raw, qty_cand),\n","            \"timestamp\": map_first_present(raw, time_cand),\n","            \"price\": map_first_present(raw, price_cand),\n","            \"user_id\": map_first_present(raw, user_cand),\n","            \"country\": map_first_present(raw, country_cand),\n","        }\n","        print_header(\"opendatabay_a | TX column mapping\")\n","        print(cols)\n","        std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","        std[\"source\"] = \"opendatabay_a\"\n","        std = ensure_cols(std, TX_TARGET)\n","        out_path = \"audit_outputs/standardized/opendatabay_a_tx.parquet\"\n","        std.to_parquet(out_path, index=False)\n","        print(\"Saved standardized:\", out_path)\n","        return std\n","    else:\n","        text_cand   = [\"text\",\"review\",\"message\",\"content\"]\n","        label_cand  = [\"label\",\"rating\",\"category\"]\n","        intent_cand = [\"intent\"]\n","        time_cand   = [\"timestamp\",\"date\",\"created_at\"]\n","        user_cand   = [\"user_id\",\"customer_id\",\"author_id\"]\n","        cols = {\n","            \"text\": map_first_present(raw, text_cand),\n","            \"label\": map_first_present(raw, label_cand),\n","            \"intent\": map_first_present(raw, intent_cand),\n","            \"slots\": None,\n","            \"conversation_id\": None, \"turn_id\": None,\n","            \"user_id\": map_first_present(raw, user_cand),\n","            \"timestamp\": map_first_present(raw, time_cand),\n","        }\n","        print_header(\"opendatabay_a | NLP column mapping\")\n","        print(cols)\n","        std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","        std[\"source\"] = \"opendatabay_a\"\n","        std = ensure_cols(std, NLP_TARGET)\n","        out_path = \"audit_outputs/standardized/opendatabay_a_nlp.parquet\"\n","        std.to_parquet(out_path, index=False)\n","        print(\"Saved standardized:\", out_path)\n","        return std\n"],"metadata":{"id":"1WQGCdq_ho8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Opendatabay consumer dataset B (schema discovery)\n","\n","def load_opendatabay_b(base=\"data/raw/opendatabay_b\"):\n","    files = glob.glob(os.path.join(base, \"*.csv\")) + glob.glob(os.path.join(base, \"*.json\"))\n","    if not files:\n","        print(f\"No files found under {base}\")\n","        return pd.DataFrame()\n","    frames=[]\n","    for f in files:\n","        if f.endswith(\".csv\"):\n","            frames.append(pd.read_csv(f, encoding_errors=\"ignore\"))\n","        else:\n","            frames.append(pd.read_json(f))\n","    raw = pd.concat(frames, ignore_index=True)\n","    raw[\"source\"] = \"opendatabay_b\"\n","    show_schema(raw, \"opendatabay_b\")\n","\n","    lower_cols = [c.lower() for c in raw.columns]\n","    looks_tx = any(k in lower_cols for k in [\"invoice\",\"invoiceno\",\"stockcode\",\"quantity\",\"unitprice\",\"customerid\",\"country\",\"transaction_date\",\"purchase_amount\"])\n","    if looks_tx:\n","        invoice_cand = [\"InvoiceNo\",\"invoice\"]\n","        item_cand    = [\"StockCode\",\"item\",\"SKU\",\"product_id\",\"Item Purchased\"]\n","        desc_cand    = [\"Description\",\"item_desc\",\"Product_Description\",\"description\",\"Item Purchased\"]\n","        qty_cand     = [\"Quantity\",\"qty\"]\n","        time_cand    = [\"InvoiceDate\",\"timestamp\",\"Date\",\"Transaction_Date\",\"Transaction Date\"]\n","        price_cand   = [\"UnitPrice\",\"price\",\"Price\",\"Purchase_Amount\",\"Purchase Amount (USD)\"]\n","        user_cand    = [\"CustomerID\",\"user_id\",\"Customer_ID\",\"Customer ID\"]\n","        country_cand = [\"Country\",\"country\",\"Location\"]\n","\n","        cols = {\n","            \"invoice\": map_first_present(raw, invoice_cand),\n","            \"item\": map_first_present(raw, item_cand),\n","            \"description\": map_first_present(raw, desc_cand),\n","            \"qty\": map_first_present(raw, qty_cand),\n","            \"timestamp\": map_first_present(raw, time_cand),\n","            \"price\": map_first_present(raw, price_cand),\n","            \"user_id\": map_first_present(raw, user_cand),\n","            \"country\": map_first_present(raw, country_cand),\n","        }\n","        print_header(\"opendatabay_b | TX column mapping\")\n","        print(cols)\n","        std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","        std[\"source\"] = \"opendatabay_b\"\n","        std = ensure_cols(std, TX_TARGET)\n","        out_path = \"audit_outputs/standardized/opendatabay_b_tx.parquet\"\n","        std.to_parquet(out_path, index=False)\n","        print(\"Saved standardized:\", out_path)\n","        return std\n","    else:\n","        text_cand   = [\"text\",\"review\",\"message\",\"content\"]\n","        label_cand  = [\"label\",\"rating\",\"category\"]\n","        intent_cand = [\"intent\"]\n","        time_cand   = [\"timestamp\",\"date\",\"created_at\"]\n","        user_cand   = [\"user_id\",\"customer_id\",\"author_id\"]\n","        cols = {\n","            \"text\": map_first_present(raw, text_cand),\n","            \"label\": map_first_present(raw, label_cand),\n","            \"intent\": map_first_present(raw, intent_cand),\n","            \"slots\": None,\n","            \"conversation_id\": None, \"turn_id\": None,\n","            \"user_id\": map_first_present(raw, user_cand),\n","            \"timestamp\": map_first_present(raw, time_cand),\n","        }\n","        print_header(\"opendatabay_b | NLP column mapping\")\n","        print(cols)\n","        std = pd.DataFrame({k: raw[v] if v in raw.columns else np.nan for k,v in cols.items()})\n","        std[\"source\"] = \"opendatabay_b\"\n","        std = ensure_cols(std, NLP_TARGET)\n","        out_path = \"audit_outputs/standardized/opendatabay_b_nlp.parquet\"\n","        std.to_parquet(out_path, index=False)\n","        print(\"Saved standardized:\", out_path)\n","        return std\n"],"metadata":{"id":"WzXHtxTqhvQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run all adapters and print/save results\n","# Adjust the base paths to your actual files.\n","\n","adapters = [\n","    (\"relational_strategies\", load_relational_strategies, \"data/raw/relational_strategies\"),\n","    (\"conversations_3k\", load_conversations_3k, \"data/raw/conversations_3k\"),\n","    (\"twitter_support\", load_twitter_support, \"data/raw/twitter_support\"),\n","    (\"sst\", load_sst, \"data/raw/sst\"),\n","    (\"snips\", load_snips, \"data/raw/snips\"),\n","    (\"topic_inputs\", load_topic_inputs, \"data/raw/topic_inputs\"),\n","    (\"reuters21578\", load_reuters, \"data/raw/reuters21578\"),\n","    (\"ecommerce\", load_ecommerce, \"data/raw/ecommerce\"),\n","    (\"opendatabay_a\", load_opendatabay_a, \"data/raw/opendatabay_a\"),\n","    (\"opendatabay_b\", load_opendatabay_b, \"data/raw/opendatabay_b\"),\n","]\n","\n","summaries = []\n","for name, fn, base in adapters:\n","    print_header(f\"Adapter: {name}\")\n","    try:\n","        df = fn(base)\n","        summaries.append({\n","            \"dataset\": name,\n","            \"rows\": len(df),\n","            \"cols\": list(df.columns),\n","            \"path\": [p for p in os.listdir(\"audit_outputs/standardized\") if p.startswith(name)]\n","        })\n","    except Exception as e:\n","        print(f\"Error in {name}: {e}\")\n","\n","pd.DataFrame(summaries)\n"],"metadata":{"id":"aMIeYaYMh5Up"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QbLBCnDAiAxd"},"execution_count":null,"outputs":[]}]}